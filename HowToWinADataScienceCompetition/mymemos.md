# 第一週
## 外れ値の扱い
非ツリーモデルで外れ値を扱う場合
- ランク変換
  - ソートされた配列内のすべての隣接オブジェクト間にランク距離を1に適用した後、外れ値は他のサンプルに非常に近くなります。
- np.log1 / (x)
  - この変換は非線形であり、外れ値を他のサンプルに比較的近づけます。
- np.sqrt(x)
  - この変換は非線形であり、外れ値を他のサンプルに比較的近づけます。
- ウィンザー化
  - winsorizationの主な目的は、フィーチャの値をクリッピングすることにより外れ値を削除することです。
  - データの外れ値を±5%タイルの絶対値が最小のもので埋める

下記処理では外れ値の距離が修正されない
- 標準化
- MinMax

ツリーモデルを使用する場合でonehotよりLEが優れている場合
- 順序のカテゴリになっている
- ラベルエンコーダーを思い付く類似した（ターゲットに関して）カテゴリに近いラベルを割り当てます。
  - ツリーはより少ない分割量で同じ品質を実現し、次に、このエンコーディングはまれなカテゴリの処理に役立ちます。
- データセット内のカテゴリフィーチャの数が膨大な場合。
  - 膨大な数の値を持つカテゴリ機能をワンホットエンコードすると、
    - （1）メモリ消費量が多い
      - スパース行列を使用する場合、最初のケースに対処できます。
    - （2）モデルで非カテゴリ機能がほとんど使用されない場合があります。
      - 使用されない特徴量が増え対応できなくなる

onehotのほうが優れいる場合
- ラベルエンコード機能へのターゲットの依存性が非常に非線形である場合、つまり、ラベルエンコード機能で互いに近い値は、閉じていないターゲット値に対応します。

カテゴリフィーチャーを線形モデルにエンコードする場合
- 基本的にonehotが優れたエンコードとなるが上記のLEが優れる場合もあるためデータセットによる場合が多い

## スケーリングについて

Z-score
- 平均:0 分散:1にスケーリングする
- Z-scoreが重要なモデル
  - データの距離が離れている(外れ値)ことを強調したい場合に有用
  - k-nearest neighbors 
  - k-means (see k-nearest neighbors)
  - logisti regression
  - linear discriminant analysis

MinMax
- データは固定範囲を0〜1にスケーリングされる
- 標準偏差が小さくなり外れ値が隠れる

Z-score vs MinMax
- アプリケーションによる
- 例
  - クラスタリング分析では、特定の距離測定に基づいてフィーチャ間の類似性を比較するために、標準化が特に重要になる場合があります。
  - 主成分分析です。ここでは、**分散を最大化する成分に関心があるため**、通常は最小-最大スケーリングよりも標準化を優先します。
  - 画像処理では特定範囲に収まるようピクセルを正規化するのにMinMaxが使われる
  - 典型的なNNは0-1スケールが必要

## NLP
n-grams
- 各単語のローカルコンテキストの利用に役立ちます。
- N-gramの特徴はまばらです。

Bag of words and Word2vec
- Bag of wordは通常、Word2vecよりも長いベクトルを生成します
- 意味的に類似した単語には通常、類似したword2vec埋め込みがあります。

## Image proc
- データセット上でどのモデルが優れているかは明確ではありません。 

# EDA

## 3step EDA
- ドメイン知識の獲得
  - データについて調べる(wikiやググる)
  - より深い問題への理解が得られる
- データが直感的かどうかを確認する
  - ドメイン知識をもとにデータを直感的に見る
  - ドメイン知識に沿ったデータか確認できる
  - テクニック
    - カラムを確認するためデータを転置してheadする
    - nunique()で各カラムの固有数が見れる
    - pd.duplicated()重複行を取得
    - pd.drop_duplicates()は重複行を削除
- データがどう生成されたか理解する
  - サンプル間隔
  - トレーニングデータと検証データを比較
  - トレーニングデータとテストデータを比較する
  - 検証データのとり方を計画できる

## 匿名データ
- テキスト
  - 単語は特定のハッシュ化をされている場合があるためそれが判明すると文章に復元できる
  - またハッシュ化してもデータ性質が崩れるわけではない
- 数値
  - 意味のない数値を最頻出や平均を削除したり差分をとると意味のある数値が浮かび上がる
何らかの関係を持っていることがある
- 特定の関係
- グループ化されている


## Visualization
- 個別データ
  - histograms
    - データ頻度や分布、最大最小が見れる
    - 外れ値を把握しやすい
  - 統計値
  - scatter index vs value
    - labelのクラス分け等が視覚化される
- データの関係性
  - ペア
    - scatter
      - データ同士の関係性の分布を見れる
    - correlation matrix
      - 相関のある特徴を俯瞰できる
  - グループ
    - Corrplot + clustering
    - plot(index vs feature statics)

## Data cleaning
- Constant features
  - trainset = testset -> remove
  - train set != test set -> 削除したときと削除しないときを比較する必要あり
- Duplicated features
- LEした結果Duplicatedになることもあるため注意が必要

- Other things to check
  - Duplicated rows
  - Check if dataset is shuffled
    - データがシャッフルされていなかったらデータリークが発生しているかもしれないため要確認
    - Biclustering
      - 行列の行と列の同時部分クラスタリングをとるためデータがシャッフルされているか確認するのに使える
      - [sklearn](https://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html)
      - [wiki](https://en.wikipedia.org/wiki/Biclustering)

## EDA examples
Technique
- df[col].factersize()
  - オブジェクト型を数値にLEしたindexと元の値を出力する
  - [factersize](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html)
- df.select_dtype(include=['object']).columns
  - object型のみのカラムを取得する
  - excludeは指定したデータ型以外のデータ型を抽出
  - [select_dtype](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html)
- 時系列データである場合それがシャッフルされていたらいかに時系列に治すかが要点となる

## Validation and overfitting
Validation
- leave-One-Out
  - 検証用に一つだけデータをとりだしてそれを何回も繰り返す方法
  - サンプルの反復：現在のサンプルを除くすべてのサンプルでモデルを再トレーニングし、現在のサンプルを予測します。モデルをN回再トレーニングする必要があります（Nがデータセット内のサンプル数の場合）。
  - 最終的に、トレインセット内のすべてのサンプルのLOO予測を取得し、損失を計算できます。
- Splitting strategies
  - rondom
    - 標準的な切り方データに特徴がなければこれで問題なし
  - Timewise
    - 時間でサンプリングされている場合は時間で区切る
  - By id
    - idは新しいデータ(クライアント等)の予測を行える用にしたいため多少idがかぶっても良いが全く新しいidは必ず必要
  - 混同したサンプリングも存在する
  - データが少ない場合は偏りに気を付ける
  - trainの分布とvalidationとtestは必ず同じような形を取らない

## Problems occurring during validation
訓練して検証してもなおテストデータとの精度差がある場合以下のことが考えられる
- データの分布が異なる(説明変数、目的変数どちらか)
  - 例: 性別の特徴量がトレーニングで男性のみテストだと女性のみ
- データが小さすぎる
- 多様で一貫性のないデータ

対策
- KFoldで平均的なスコアを検証する
  - 5kがよく使われる
- 一方の分割でモデルを調整し、もう一方の分割でスコアを評価します

kaggleのLBから読み取れること
- LBスコアと検証スコアが一貫して高い/低いか
- LBスコアが検証スコアと相関していないか
  - kfoldですでにデータが偏っている
  - public LBのデータ量が小さすぎる
  - トレーニングとテストのデータが違う分布を取っている

検証で発生する問題
- public LBのデータが小さすぎる
- train/testが誤った区切り方をされている
- train/testでデータの分布が違う

## Summary of Validation topic
1. 検証を定義することでOverfittingを把握できる
2. 共通の検証の戦略を説明できるようにする
3. 主なデータのsplitを試す
4. 主な検証の問題にどう取り組み解析するか

## Data leaks
データリークは現実的には役に立たないモデルを作成するがコンペのゴールは最も精度を高くすることなのでデータリークを悪用することも十分必要

予期しないデータリーク
- メタデータ
  - 画像系で画像フォルダのzipの日付だけ与えることでほぼ完璧なスコアを獲得できる
- ID等に情報を含めてしまう
  - 何らかのハッシュをIDとして扱った場合規則性をもつことがある
- indexの並び順
  - ターゲットの並び順に意味がある場合にインデックス番号をふるだけで精度が改善する

LBの調査
1. リーダーボードの公開部分からすべてのグラウンドトゥルースを抽出することです
2. publicとprivateの精度変化を調査する
  - 全ターゲットを一意のデータにしてpublic/privateの変化を観察
  - 確実性の高いカテゴリには同じラベルがつけられる
  - N1(ターゲットが1の行数) / N(実際の行数) = (-L(一定の予測で得られるLBスコア) - ln(1-C)) / (lnC - ln(1-C))この式に沿ってテストとpublic/privateの違いを把握できた


# Metrics optimization
- なぜ評価指標は多く存在するか?
  - 評価指標はアルゴリズムの等価性を測定する方法であるが特定の問題に適した評価指標が存在するためいくつも存在する
- なぜ気を付けるべきか?
  - コンペの順位上げに関わるから

評価指標について
- どう機能するか
- どう最適化されるか

探索的なメトリック分析を行うことは異常なメトリックを見つけるのとメトリックの最適化に有用

## Regression metric
- MSE, RMSE, R-squared
  - 同様の最適化が行える
  - 式がMSEが支配的なためMSEを最適化できれば残りの評価指標も最適化できる
- MAE
  - MSEよりロバストである
  - ハズレ値に対する感度が低い
  - ハズレ値に近い正常値も扱いたい場合は向いていない
  - 説明しやすい
- (R)MSPE, MAPE
  - MSE, MAEともにデータ量に応じた誤差の大きさを表現できないためこの指標が必要
  - 相対エラー
  - MSE, MAEの加重の効果を足したもの
  - MSPEは平均を使う
  - MAPEは中央値を使う
- (R)MSLE　
  - MSPE, MAPEと同様の状況で使われる
  - 小さなターゲットへの偏りが少なくMAPEより優れている部分が多い

## Classification metric
- Accuracy, LogLoss, AUC
  - Accuracy
    - Accuracyは0,1なのでデータによってはモデルが1しか予測しなくても良いスコアを出す可能性がある
    - 不均衡なデータ・セットに向いていない
  - Precision
    - マルチ分類に向いている
    - 不均衡なデータ・セットに向いている
  - Recall
    - マルチラベル/マルチクラス分類および情報検索に適しています
    - 不均衡なデータセットに適しています
  - F1
    - PrecisionとRecallのバランスをとるのに使える
    - 精度とリコールに等しい重みを与えます
    - 不均衡なデータセットに適しています
  - LogLoss
    - 分類するべきクラスに属する確率で評価する指標
    - 2値分類およびマルチクラス分類問題にて利用できる (計算式は異なる)
    - すべてのクラスに属する確率の合計は、常に 1
    - 間違った分類に対して大きなペナルティ(指数関数的に増加する) を課す
  - AUC
    - 予測すべき空間をどれだけフォローできたかの面積が指標になる
    - バイナリ問題のみ
      - Accuracyと同じ問題がある
    - 0-1までをとる
    - 間違った値をとるとその分面積が減る
      - TP/FPRateがでる
    - ランクベースのメトリック
    - モデルがTRUE値とFALSE値をどれだけうまく区別できるかを推定する必要がある場合に適しています
      - つまり、インスタンスAとBのすべてのペアについて、AがBよりも高い可能性があるとモデルが言う場合、Aが実際にTRUEおよびB FALSEである頻度はどれくらいですか？
  - Gini coefficient
    - Gini係数は、モデルがTRUE値とFALSE値を区別する度合いを測定するために使用できるランクベースのメトリックであるため、AUCスコアに匹敵します。
    - 範囲は0から1.0で、これは完全な差別に相当します。
- Cohen's(Quadratic weighted) Kappa
  - 重みのマトリックスを設定することで特定のタスクの分類に特定のペナルティを課せれる
  - kaggleで一般的に用いられる指標

## Loss and metric
- metric
  - スコアの最適化
  - モデルの結果にたいする評価
- Loss
  - モデルの最適化につかう

場合によってはmetric = Lossとなることもある

主なアプローチ
- 正しいモデルを使う
- トレーニングデータの前処理とメトリックの最適化
- メトリックを最適化して実際のスコアを予測する
- カスタムLoss functionを作る
- 最適化されたmetricに早期終了を使う

## Regression metric optimization
早期終了は常に使える

MSE
- 使うモデルの損失関数をMSEにする
- 小さい誤差を計算するときに用いる

MAE
- モデルによっては使えないが使えるモデルでの損失関数に適用する
- 実行時間に難あり
- Huber Loss(MSEとMAEを混ぜたもの)という損失関数が実装されている場合がありこれはエラーが大きい場合にMAEと似た動きをする
- 分位損失という名前で実装されていることもある
- NNについては自分で実装する形になる(あまり難しくない)
- 大きな誤差に対応できる

MSPE, MAPEは上記のもののデータ量を考慮したものなので大きな性質差は無い

RMSLE
- トレーニングはyを最適化しテスト時はy^を最適化する

## Classification metrics optimization
- logloss
  - 多くのモデルに実装されているためそれを損失関数として使用する
  - RandomForestでは何もしないでこれを最適化するのは向いていない
- Accuracy
  - どのモデルでも実装されている
  - 早期終了を行ったりする
- AUC
  - 勾配ベースでAUCを実装されているモデルは適切なパラメータを設定する必要がある
  - ペアワイズロス
    - ペアワイズ損失は、オブジェクトのペアの予測とラベルを取得し、それらの損失を計算します。順序が正しい場合、損失はゼロであることが理想的であり、順序が正しくない、正しくない場合、損失はゼロよりも大きいことが理想です。
    - loglossなどの実際の損失関数を使用できる
- Kappa
  - MSEが式にあるためそれを最適化する
  - Bad
    - 予測値を丸める
  - Better
    - 閾値を最適化する

## 決定木におけるGini vs Entropy
- Giniは連続属性を対象としており、クラスで発生する属性はエントロピーを対象としています
- 「Gini」は最大のクラスを見つける傾向があり、「エントロピー」はデータの最大50％を構成するクラスのグループを見つける傾向があります
- 誤分類を最小限に抑える「Gini」
- 探索的分析のための「エントロピー」
- いくつかの研究では、これは重要ではないことが示されています。これらの違いは2％未満です
- エントロピーは計算が少し遅いかもしれません

## Mean Encodings
- カテゴリ変数のエンコード法の一種
- 目的変数を元にカテゴリがまとまっている目的変数の平均を取りその値でエンコードする
[参考URL](https://qiita.com/suaaa7/items/cfe9a9e516b5b784570f)

なぜ機能するか?
- LEでは論理順序がないためランダムなエンコードとなる
- これをターゲットデータを使ってエンコードすることで論理順序を活かすことができる
- よって特徴量と目的変数の関係が複雑かつ非線形であるほどより効果を発揮する
- 妥当な数値となっているかは検証が必要
  - 0以外のデータと0のデータの割合を取って検証する
- リーク防止
  - CVを使って複数回検証データからトレーニングデータの各カラム平均をマッピングする最後に欠損値は目的変数の平均で埋める
  - 平均エンコーディングのカラムにLOOを使ってリークを防ぐ
  - 正則化はリーク防止に役立つ
    - 平滑化で平均エンコーディングを行う
  - ランダムノイズを追加する
  - 拡張平均
    - 各カラムをgroupbyした目的変数の累積和 / 各カラムをgroupbyした目的変数のカウント
    - これにより、ターゲット変数からの漏れが最小限に抑えられ、調整のためにハイパーパラメーターを必要としません。欠点は、エンコード品質が不規則であるということです。 CatBoostライブラリには組み込みの実装があります。
- 回帰
  - ターゲット変数の中間、
  - パーセンタイル、
  - 標準偏差
  - ビンを計算することもできます。
    - たとえば、ターゲット変数が1〜100の範囲に分布している場合、10個のビンフィーチャを作成できます。最初の機能では、1〜10の間でターゲットになったデータポイントの数をカウントし、2番目は10〜20の間などにカウントします。
- マルチタスク
  - 相互作用のある特徴量をツリーで探索して高いもののエンコードを行っていく

# Hyper parameter tuning
ハイパーパラメータのチューニング方法
- 最も影響のあるパラメータを探すして絞る
  - 各フレームワークやライブラリのドキュメントを読む
- パラメータ変更時の変化を理解する
- 最適なパラメータが見つかるまでイテレーションしていく

ハイパーパラメータ最適化ライブラリの活用も重要

アンダーフィティングとオーバーフィッテイングに気を付ける

木ベースモデル
- GBDT
  - XGBoost
  - LightGBM
  - CatBoost
- RandomForest, ExtraTrees
  - scikit-learn
- Other
  - RGF

# Tips and Tricks
コンペに参加する前に自分のゴールを定義する。
- より面白い問題に取り組む
- 新しいツールに出会う
- メダルの獲得
  - 提出数の多いものや少数のグループのみがリーダーボードを占めているときは難易度が高いということ
  - ソロがリーダーボードに多くかつ提出数も少ない場合はチームを組んで取り組むと良い確率でメダルが獲得できる

コンペ参加後

フォーラムを読んだり議論について注目することでいくつかのアイデアを取得できる。
- アイデアを構造的に整理する
- 重要か有望なアイデアを選択する
- なぜうまく行かないか考察する

ハイパーパラメータについて
全パラメータを以下の形に並び替える
- 重要性
- 実現可能性
- 理解度


データ読み込み
- データ前処理後はcsv/txtは再読込高速化のためhdf5/npyにコンバートする
- 64bitから32bitにダウンキャストするのも一つの有効な手段
- huge dataはチャンク分割を活用する

パーフォマンス評価
- 精度の向上に行き詰まらない限りはCVを使わずにtrain/testで検証を行う
- とりあえずLightGBMを使ってみる

早く汚くいつもよく
- コードのクウォリティを気にしすぎない
- 思考を単純に必要なものだけ保存する
- PCスペックに不満を感じたらスペックを上げる

初期のpipeline
- シンプルな形で始める
- パイプラインの完全デバック
- 単純な状態から複雑性を足していく
  - はじめはRFで始める

良いソフトウェア開発
- わかりやすい変数名
- 再現性を担保する
- コードを再利用可能な状態にしておく

記事を読む
- 次に使えるアイデアを獲得できる

行動としてのパイプライン
- フォーラムやカーネルを読む
  - データの勘所等を早くつかめる
- EDAとベースライン作成に取り組む
- 特徴量の追加を行う
- ハイパーパラメータ最適化
- submissionノートブックを分ける
- jupyter macro機能を活用して初期設定を簡略化する

# Advaced features
- チャンク化できる特徴量を元にそれ以外の特徴量の統計値を計算したものを新たな特徴量とする。
-行列因子分解
  - SVD, PCA
  - TruncatedSVD
    - テキストデータが得意
  - NMF
    - 決定木に合わせた状態に変換する
- 特徴量の結合
  - カテゴリ値であれば文字列の連結
  - 数値であれば数式を使った結合
- t-SNE
  - 多様体学習の一つ

# Ensembling
様々なモデルをアンサンブルしてより良いモデルを作成する

アンサンブルタイプ
- Averaging or blending
  - 混ぜるmodelの平均精度が精度となる
- Weighted averaging
  - 加重平均を取ったもの
- Conditional averaging
  - 条件付きで平均法をとる
- Bagging
  - 同じモデルのわずかに異なるバージョンを平均化することを指します
  - Random Forestがうまく行ったアルゴリズムの一例
  - 僅かに異なるバージョンとはモデルのランダムシードやブートストラップを変更することが当たる
- Boosting
  - モデルの加重平均の形式で、各モデルは以前のモデルのパフォーマンスを考慮した方法で順次構築されます。
  - より良い予測を行うために以前のモデルがどれだけうまく行ったかを考慮しています
  - 2つの主要なブースティングタイプ
    - 重量
      - 重みつけの流れ
        - 重みつけ変数を設定
        - 予測
        - 予測と正解の絶対誤差を取る
        - 絶対誤差に1足したものを重みとする
        - 学習率を用いてパラメータを更新していく
    - 残留誤差
  - XGBoost, LightGBM, CatBoostがよりboostingを活用出来ている
- Stacking
  - 基本的に、ホールドアウトデータセットを使用していくつかの予測を行うことを意味します。そして、これらの予測を収集またはスタックして、新しいデータセットを形成します。新しいデータセットでは、予測からこの新しく形成されたデータセットに新しいモデルを適合させることができます。
  - 主なロジックは、ランダムフォレストと線形回帰の2つの基本学習器を入力データに使用すること
  - このモデルを、検証データと検証データのターゲットで行われた予測に適合させました。
  - 
- StackNet
  - 複数レベルのニューラルネットワークアーキテクチャで複数のモデルを組み合わせるためにスタッキングを利用するスケーラブルなメタモデリング手法
  - 同じレベル内ですべてのモデルを並行して実行できるため、スケーラブルです
  - 前に述べたこの手法を使用してデータを分割し、データを保持するために予測を行い、別のモデルを使用してそれらの予測を訓練するため、スタックを利用します
  - Kfoldで活用できるデータの組み合わせを拡張する
- Ensembling Tips and Tricks
  - スタックについて説明したときに、非常に重要なのは多様性を導入することです
- metamodelの検証
  - 

# Competitions go through
## Crowedflower Competition
コンテスト内容
- コンペティションの目標は、クエリと、生きているeコマースサイトから得られた製品の説明に基づいて、検索結果の関連性を測定することです。
- タスクは、検索アルゴリズムの精度を評価することです。
- 競争の課題は、特定のクエリと問題の説明の関連性スコアを予測することです。
データについて
- リクエストクエリ、
- 結果と製品のタイトル、
- 製品の説明の3つのテキストフィールド、
- およびスコアのターゲット、
  - 中央値、分散に関連する2つの列で構成されます。
指標
- 二次加重カッパ
  - 2つの評価間の一致を測定する
  - 通常、0（評価者間のランダムな合意）から1、評価者間の完全な合意まで上昇します。
  - kappaには、分類損失と回帰損失の両方に類似した特性がある
  - 予測された評価と真の評価が遠ければ遠いほど、ペナルティが多くなります

基礎的なソリューションの要約
- Text features: similarities
  - 単語カウント
  - TF-IDFのコサイン距離
  - word2vecの距離平均の間
  - レーベンシュタイン距離
- Text features: symbolic n-grams
  - n-gramを1-5まで試した
- Sample weighting
- Bumper features
  - 機能としてクラス間に人工的に作成されたいくつかのバイナリ区切り文字を追加することにしました。
  - ターゲットクラス番号が1よりも大きいこと、2よりも大きいことなどです
  - クラス間のセパレータのようなものである
上記4つの方法で近似度をまず確認した
クエリのタイトルとクエリの説明のペアの場合、マジックワードの数、TF-IDF表現間のコサイン距離、平均word2vecベクトル間の距離、Levensthein距離を計算しました。

シンボリックn-gramを使用すると便利であることがわかりました。単語と同じ方法でそれらを使用できます

各文字が個別の単語として解釈される場合に基づきます。 1文字から5文字までのシンボリックn-gramを使用します。

- 拡張クエリ
  - クエリごとに、最も関連性の高い対応するアイテム、関連度4のアイテムを取得します
  - 関連するアイテムのタイトルからすべての単語を結合し、最も人気のある10個の単語を取得します

応用的なソリューション

# 参考
- [【随時更新】Kaggleテーブルデータコンペできっと役立つTipsまとめ](https://naotaka1128.hatenadiary.jp/entry/kaggle-compe-tips)
- [Mean Encoding](https://necromuralist.github.io/kaggle-competitions/posts/mean-encoding/)
- [courseraのKaggle講座内容まとめ](https://qiita.com/saitosasaki/items/da2a0f6ad2cc5c5ea7a2)
- [KAGGLE ENSEMBLING GUIDE](https://mlwave.com/kaggle-ensembling-guide/)
